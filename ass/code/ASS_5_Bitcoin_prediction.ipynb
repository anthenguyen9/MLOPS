{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 31857,
          "databundleVersionId": 2820378,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Từ đây xuống bỏ"
      ],
      "metadata": {
        "id": "FiS-z0g6hPQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bài hoàn chỉnh"
      ],
      "metadata": {
        "id": "w58Lu0nYhPQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Analysts@Emory — BTC next-hour up/down (±0.01%) classifier (improved)\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "\n",
        "# -------------------------\n",
        "# 1) ĐỌC DỮ LIỆU\n",
        "# -------------------------\n",
        "CANDIDATE_PATHS = [\n",
        "    \"/content/data/input/emory/BTC_USDT_1h.csv\"\n",
        "]\n",
        "\n",
        "data_path = None\n",
        "for p in CANDIDATE_PATHS:\n",
        "    if os.path.exists(p):\n",
        "        data_path = p\n",
        "        break\n",
        "if data_path is None:\n",
        "    raise FileNotFoundError(\"Không tìm thấy BTC_USDT_1h.csv trong Input.\")\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# -------------------------\n",
        "# 2) PARSE TIME\n",
        "# -------------------------\n",
        "if \"unix\" in df.columns:\n",
        "    mx = pd.to_numeric(df[\"unix\"], errors=\"coerce\").max()\n",
        "    unit = \"ms\" if mx > 1e12 else \"s\"\n",
        "    df[\"date\"] = pd.to_datetime(df[\"unix\"], unit=unit, errors=\"coerce\")\n",
        "else:\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"mixed\", errors=\"coerce\")\n",
        "\n",
        "df = df.dropna(subset=[\"date\"]).sort_values(\"date\").set_index(\"date\")\n",
        "\n",
        "rename_map = {}\n",
        "for c in df.columns:\n",
        "    if c.strip().lower() == \"volume btc\":\n",
        "        rename_map[c] = \"volume_btc\"\n",
        "    if c.strip().lower() == \"volume usdt\":\n",
        "        rename_map[c] = \"volume_usdt\"\n",
        "df = df.rename(columns=rename_map)\n",
        "\n",
        "# -------------------------\n",
        "# 3) TARGET\n",
        "# -------------------------\n",
        "df[\"ret_fwd1\"] = df[\"close\"].pct_change(periods=1).shift(-1)\n",
        "thr = 1e-4\n",
        "df[\"target\"] = np.where(df[\"ret_fwd1\"] >= thr, 1,\n",
        "                        np.where(df[\"ret_fwd1\"] <= -thr, 0, np.nan))\n",
        "df = df.dropna(subset=[\"target\"])\n",
        "\n",
        "print(\"Target distribution:\")\n",
        "print(df[\"target\"].value_counts().rename_axis(\"target\").to_frame(\"count\"))\n",
        "print((df[\"target\"].value_counts(normalize=True)).rename(\"proportion\"))\n",
        "\n",
        "# -------------------------\n",
        "# 4) FEATURE ENGINEERING\n",
        "# -------------------------\n",
        "def add_features(frame, prefix=\"btc\"):\n",
        "    c = \"close\"; h=\"high\"; l=\"low\"; o=\"open\"\n",
        "    v1 = \"volume_btc\"; v2 = \"volume_usdt\"\n",
        "    out = pd.DataFrame(index=frame.index)\n",
        "\n",
        "    if c in frame.columns:\n",
        "        logp = np.log(frame[c].clip(lower=1e-9))\n",
        "        out[f\"{prefix}_ret_1\"]   = logp.diff(1)\n",
        "        out[f\"{prefix}_ret_3\"]   = logp.diff(3)\n",
        "        out[f\"{prefix}_ret_6\"]   = logp.diff(6)\n",
        "        out[f\"{prefix}_ret_12\"]  = logp.diff(12)\n",
        "        out[f\"{prefix}_ma_6\"]    = frame[c].rolling(6).mean()\n",
        "        out[f\"{prefix}_ma_12\"]   = frame[c].rolling(12).mean()\n",
        "        out[f\"{prefix}_std_6\"]   = frame[c].rolling(6).std()\n",
        "        out[f\"{prefix}_std_12\"]  = frame[c].rolling(12).std()\n",
        "\n",
        "        # RSI(14)\n",
        "        delta = frame[c].diff()\n",
        "        up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
        "        rs = up.rolling(14).mean() / (down.rolling(14).mean() + 1e-9)\n",
        "        out[f\"{prefix}_rsi_14\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # MACD\n",
        "        ema12 = frame[c].ewm(span=12, adjust=False).mean()\n",
        "        ema26 = frame[c].ewm(span=26, adjust=False).mean()\n",
        "        macd = ema12 - ema26; signal = macd.ewm(span=9, adjust=False).mean()\n",
        "        out[f\"{prefix}_macd\"] = macd\n",
        "        out[f\"{prefix}_macd_sig\"] = signal\n",
        "        out[f\"{prefix}_macd_hist\"] = macd - signal\n",
        "\n",
        "        # Candle shape features\n",
        "        if all(k in frame.columns for k in [o,h,l,c]):\n",
        "            out[f\"{prefix}_hl\"] = frame[h] - frame[l]\n",
        "            out[f\"{prefix}_co\"] = frame[c] - frame[o]\n",
        "            out[f\"{prefix}_upper_shadow\"] = frame[h] - frame[[c,o]].max(axis=1)\n",
        "            out[f\"{prefix}_lower_shadow\"] = frame[[c,o]].min(axis=1) - frame[l]\n",
        "\n",
        "        # Volatility cluster\n",
        "        ret2 = out[f\"{prefix}_ret_1\"]**2\n",
        "        out[f\"{prefix}_vol_cluster_12\"] = ret2.rolling(12).mean()\n",
        "        out[f\"{prefix}_vol_cluster_24\"] = ret2.rolling(24).mean()\n",
        "\n",
        "    # volumes\n",
        "    if v1 in frame.columns:\n",
        "        out[f\"{prefix}_vol_btc\"] = frame[v1]\n",
        "        out[f\"{prefix}_vol_btc_chg\"] = frame[v1].pct_change(1)\n",
        "    if v2 in frame.columns:\n",
        "        out[f\"{prefix}_vol_usdt\"] = frame[v2]\n",
        "        out[f\"{prefix}_vol_usdt_chg\"] = frame[v2].pct_change(1)\n",
        "\n",
        "    # time features\n",
        "    out[\"hour\"] = out.index.hour\n",
        "    out[\"dow\"]  = out.index.dayofweek\n",
        "    return out\n",
        "\n",
        "X_all = add_features(df, \"btc\")\n",
        "XY = pd.concat([X_all, df[\"target\"]], axis=1).replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y = XY[\"target\"].astype(int)\n",
        "X = XY.drop(columns=[\"target\"])\n",
        "\n",
        "imp = SimpleImputer(strategy=\"median\")\n",
        "X_imp = pd.DataFrame(imp.fit_transform(X), index=X.index, columns=X.columns)\n",
        "print(\"Data ready:\", X_imp.shape, \"| positives:\", int(y.sum()), \"| negatives:\", int((1-y).sum()))\n",
        "\n",
        "# -------------------------\n",
        "# 5) CV + LightGBM\n",
        "# -------------------------\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "oof_pred_bin = pd.Series(index=y.index, dtype=float)\n",
        "oof_pred_proba = pd.Series(index=y.index, dtype=float)\n",
        "f1_scores = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(tscv.split(X_imp), 1):\n",
        "    X_tr, X_va = X_imp.iloc[tr_idx], X_imp.iloc[va_idx]\n",
        "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "\n",
        "    model = lgb.LGBMClassifier(\n",
        "        n_estimators=3000,\n",
        "        learning_rate=0.02,\n",
        "        num_leaves=63,\n",
        "        max_depth=-1,\n",
        "        min_child_samples=50,\n",
        "        subsample=0.7,\n",
        "        subsample_freq=1,\n",
        "        colsample_bytree=0.7,\n",
        "        reg_alpha=0.5,\n",
        "        reg_lambda=1.0,\n",
        "        objective=\"binary\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric=\"binary_logloss\",\n",
        "        callbacks=[early_stopping(100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    val_proba = model.predict_proba(X_va)[:, 1]\n",
        "    thr_grid = np.linspace(0.05, 0.95, 91)  # chi tiết hơn\n",
        "    best_f1, best_thr = -1, 0.5\n",
        "    for t in thr_grid:\n",
        "        f1 = f1_score(y_va, (val_proba >= t).astype(int))\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_thr = f1, t\n",
        "\n",
        "    oof_pred_proba.iloc[va_idx] = val_proba\n",
        "    oof_pred_bin.iloc[va_idx]   = (val_proba >= best_thr).astype(int)\n",
        "    f1_scores.append(best_f1)\n",
        "    print(f\"Fold {fold}: best F1={best_f1:.4f} @thr={best_thr:.2f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation\n",
        "# -------------------------\n",
        "valid_idx = oof_pred_bin.dropna().index\n",
        "print(\"\\nMean F1 (OOF):\", np.mean(f1_scores).round(4))\n",
        "print(classification_report(y.loc[valid_idx],\n",
        "                            oof_pred_bin.loc[valid_idx].astype(int)))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-16T15:58:24.249887Z",
          "iopub.execute_input": "2025-08-16T15:58:24.250382Z",
          "iopub.status.idle": "2025-08-16T15:58:33.378907Z",
          "shell.execute_reply.started": "2025-08-16T15:58:24.250347Z",
          "shell.execute_reply": "2025-08-16T15:58:33.377758Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCDLDS-QhPQG",
        "outputId": "80048f19-5486-4b6d-f307-5df0e7f771b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target distribution:\n",
            "        count\n",
            "target       \n",
            "1.0     19765\n",
            "0.0     18732\n",
            "target\n",
            "1.0    0.513417\n",
            "0.0    0.486583\n",
            "Name: proportion, dtype: float64\n",
            "Data ready: (38451, 24) | positives: 19734 | negatives: 18717\n",
            "[LightGBM] [Info] Number of positive: 3338, number of negative: 3073\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5619\n",
            "[LightGBM] [Info] Number of data points in the train set: 6411, number of used features: 23\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.520668 -> initscore=0.082718\n",
            "[LightGBM] [Info] Start training from score 0.082718\n",
            "Fold 1: best F1=0.6729 @thr=0.41\n",
            "[LightGBM] [Info] Number of positive: 6574, number of negative: 6245\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003035 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5625\n",
            "[LightGBM] [Info] Number of data points in the train set: 12819, number of used features: 23\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.512833 -> initscore=0.051341\n",
            "[LightGBM] [Info] Start training from score 0.051341\n",
            "Fold 2: best F1=0.6788 @thr=0.05\n",
            "[LightGBM] [Info] Number of positive: 9866, number of negative: 9361\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004607 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5633\n",
            "[LightGBM] [Info] Number of data points in the train set: 19227, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513133 -> initscore=0.052542\n",
            "[LightGBM] [Info] Start training from score 0.052542\n",
            "Fold 3: best F1=0.6783 @thr=0.33\n",
            "[LightGBM] [Info] Number of positive: 13149, number of negative: 12486\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005959 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5636\n",
            "[LightGBM] [Info] Number of data points in the train set: 25635, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.512932 -> initscore=0.051738\n",
            "[LightGBM] [Info] Start training from score 0.051738\n",
            "Fold 4: best F1=0.6870 @thr=0.37\n",
            "[LightGBM] [Info] Number of positive: 16496, number of negative: 15547\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007667 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 5641\n",
            "[LightGBM] [Info] Number of data points in the train set: 32043, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.514808 -> initscore=0.059250\n",
            "[LightGBM] [Info] Start training from score 0.059250\n",
            "Fold 5: best F1=0.6737 @thr=0.38\n",
            "\n",
            "Mean F1 (OOF): 0.6781\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.04      0.07     25088\n",
            "           1       0.52      0.99      0.68     26304\n",
            "\n",
            "    accuracy                           0.52     51392\n",
            "   macro avg       0.62      0.51      0.37     51392\n",
            "weighted avg       0.61      0.52      0.38     51392\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ===== Config =====\n",
        "predict_path = \"/content/data/input/emory/predictions.csv\"   # TODO: đổi path file predict gốc (vd: \"/mnt/data/annt43_predict.csv\")\n",
        "out_dir = Path(\"/content/data/output\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "TARGET_ROWS = 39392\n",
        "THRESH = 0.5  # ngưỡng nhị phân cho dự đoán từ model\n",
        "FMT = '%m/%d/%Y %H:%M'\n",
        "REGEX = r'^(0[1-9]|1[0-2])/(0[1-9]|[12]\\d|3[01])/\\d{4} (?:[01]\\d|2[0-3]):[0-5]\\d$'\n",
        "\n",
        "def normalize_date_text(values):\n",
        "    \"\"\"\n",
        "    Nhận Series hoặc Index, chuẩn hoá chuỗi thời gian lạ (ví dụ '2017-08-31 03-PM', '03PM', ...),\n",
        "    parse -> datetime, làm tròn đến phút. Trả về cùng \"kiểu\": nếu input là Index -> DatetimeIndex,\n",
        "    nếu input là Series -> Series[datetime64[ns]].\n",
        "    \"\"\"\n",
        "    # Luôn làm việc trên Series dạng string để replace dễ\n",
        "    s = pd.Series(values, copy=False).astype(str).str.strip()\n",
        "\n",
        "    # Chuẩn hoá các biến thể AM/PM\n",
        "    s = s.str.replace(r'(\\d{1,2})-(AM|PM)\\b', r'\\1:00 \\2', regex=True, flags=re.IGNORECASE)\n",
        "    s = s.str.replace(r'\\b(\\d{1,2})\\s*(AM|PM)\\b', r'\\1:00 \\2', regex=True, flags=re.IGNORECASE)\n",
        "    s = s.str.replace(r'\\b(\\d{1,2}):(\\d{2})(AM|PM)\\b', r'\\1:\\2 \\3', regex=True, flags=re.IGNORECASE)\n",
        "\n",
        "    # Parse ưu tiên MM/DD, sau đó thử dayfirst để cứu trường hợp hiếm\n",
        "    dt = pd.to_datetime(s, errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
        "    miss = dt.isna()\n",
        "    if miss.any():\n",
        "        dt2 = pd.to_datetime(s[miss], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
        "        dt[miss] = dt2\n",
        "\n",
        "    # Làm tròn đến phút\n",
        "    dt = dt.dt.floor('min')\n",
        "\n",
        "    # Trả về cùng \"kiểu\" với input\n",
        "    if isinstance(values, pd.Index):\n",
        "        return pd.DatetimeIndex(dt)\n",
        "    else:\n",
        "        return dt\n",
        "\n",
        "\n",
        "def parse_0_1_or_nan(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Trả về 0/1 nếu giá trị đúng 0 hoặc 1; ngược lại -> NaN (coi như 'chưa predict được').\"\"\"\n",
        "    vals = pd.to_numeric(series, errors='coerce')\n",
        "    out = pd.Series(np.nan, index=series.index, dtype='float64')\n",
        "    out[(vals == 0) | (vals == 1)] = vals[(vals == 0) | (vals == 1)]\n",
        "    return out\n",
        "\n",
        "def model_predict_binary(X) -> np.ndarray:\n",
        "    \"\"\"Trả về vector 0/1 từ model (support cả classifier có predict_proba và regressor).\"\"\"\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(X)[:, 1]\n",
        "        return (proba >= THRESH).astype(int)\n",
        "    raw = np.asarray(model.predict(X)).ravel()\n",
        "    return raw.astype(int) if set(np.unique(raw)).issubset({0,1}) else (raw >= THRESH).astype(int)\n",
        "\n",
        "# ===== 1) Đọc file predict gốc =====\n",
        "pred_src = pd.read_csv(predict_path)\n",
        "\n",
        "# Xác định cột date (ưu tiên tên 'date', nếu không có thì lấy cột đầu tiên)\n",
        "date_col = 'date' if 'date' in pred_src.columns else pred_src.columns[0]\n",
        "dates_parsed = normalize_date_text(pred_src[date_col])\n",
        "\n",
        "# Xác định cột dự đoán của file gốc (ưu tiên '0.02%', sau đó 'target', nếu không thì cột thứ 2)\n",
        "if '0.02%' in pred_src.columns:\n",
        "    src_pred_col = '0.02%'\n",
        "elif 'target' in pred_src.columns:\n",
        "    src_pred_col = 'target'\n",
        "elif len(pred_src.columns) >= 2:\n",
        "    src_pred_col = pred_src.columns[1]\n",
        "else:\n",
        "    raise ValueError(\"Không tìm thấy cột dự đoán trong file predict (cần >= 2 cột).\")\n",
        "\n",
        "# Giá trị cũ hợp lệ (0/1) -> giữ; còn lại (NaN/khác 0-1) -> coi là thiếu để điền bằng model\n",
        "src_01 = parse_0_1_or_nan(pred_src[src_pred_col])\n",
        "\n",
        "# ===== 2) Chuẩn hoá index của X_imp để map theo phút =====\n",
        "X_imp_norm = X_imp.copy()\n",
        "X_imp_norm.index = normalize_date_text(pd.Index(X_imp_norm.index))\n",
        "\n",
        "# Tập mốc thời gian có trong file predict\n",
        "need_dates = pd.to_datetime(dates_parsed.dropna().unique())\n",
        "X_test = X_imp_norm.loc[X_imp_norm.index.isin(need_dates)].copy()\n",
        "\n",
        "# ===== 3) Dự đoán từ model & tạo map theo date =====\n",
        "if len(X_test) > 0:\n",
        "    model_bin = model_predict_binary(X_test)\n",
        "    pred_map = pd.Series(model_bin, index=pd.to_datetime(X_test.index))\n",
        "    # nếu có duplicate index, lấy cái cuối\n",
        "    pred_map = pred_map[~pred_map.index.duplicated(keep='last')]\n",
        "else:\n",
        "    pred_map = pd.Series(dtype='float64')\n",
        "\n",
        "# ===== 4) Hợp nhất: giữ giá trị cũ 0/1; chỗ nào NaN thì điền bằng model; còn thiếu -> 0\n",
        "final_vals = src_01.copy()\n",
        "mask_missing = final_vals.isna()\n",
        "\n",
        "if mask_missing.any():\n",
        "    # lấy dự đoán theo date tương ứng\n",
        "    fill_dates = pd.to_datetime(dates_parsed[mask_missing], errors='coerce')\n",
        "    fill_from_model = []\n",
        "    for ts in fill_dates:\n",
        "        if pd.isna(ts) or (ts not in pred_map.index):\n",
        "            fill_from_model.append(np.nan)\n",
        "        else:\n",
        "            fill_from_model.append(int(pred_map.loc[ts]))\n",
        "    final_vals.loc[mask_missing] = fill_from_model\n",
        "\n",
        "# chỗ nào vẫn NaN (không map được model) -> 0\n",
        "final_vals = final_vals.fillna(0).astype(int)\n",
        "\n",
        "# ===== 5) Lập DataFrame output với format date chuẩn & đúng header =====\n",
        "out = pd.DataFrame({\n",
        "    'date': pd.to_datetime(dates_parsed, errors='coerce').dt.strftime(FMT),\n",
        "    '0.02%': final_vals.values\n",
        "})\n",
        "\n",
        "# ===== 6) Kiểm tra format date toàn bộ & đảm bảo đúng 39,392 dòng =====\n",
        "ok_regex = out['date'].str.match(REGEX, na=False)\n",
        "ok_parse = pd.to_datetime(out['date'], format=FMT, errors='coerce').notna()\n",
        "bad_mask = ~(ok_regex & ok_parse)\n",
        "if bad_mask.any():\n",
        "    print(\"❌ Ví dụ date sai format:\")\n",
        "    print(out.loc[bad_mask, 'date'].head(10).to_string(index=False))\n",
        "    raise ValueError(\"Date format invalid. Yêu cầu 'MM/DD/YYYY HH:MM' (vd: 10/13/2021 03:00).\")\n",
        "\n",
        "if len(out) > TARGET_ROWS:\n",
        "    out = out.iloc[:TARGET_ROWS].copy()\n",
        "elif len(out) < TARGET_ROWS:\n",
        "    raise ValueError(f\"Row count {len(out)} < {TARGET_ROWS}. File predict gốc phải đủ {TARGET_ROWS} dòng.\")\n",
        "\n",
        "# ===== 7) Lưu =====\n",
        "save_path = out_dir / \"submission.csv\"\n",
        "out.to_csv(save_path, index=False)\n",
        "print(f\"✅ Saved {save_path} | rows={len(out)} | header=['date','0.02%'] | format={FMT} | kept old 0/1, filled missing via model, else 0\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BWxqihwnrhR",
        "outputId": "4b561a77-09c0-4482-dd6f-cb1ddfa0ed6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2320659374.py:30: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
            "/tmp/ipython-input-2320659374.py:33: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt2 = pd.to_datetime(s[miss], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
            "/tmp/ipython-input-2320659374.py:33: UserWarning: Parsing dates in %Y-%m-%d %I:%M:%S %p format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
            "  dt2 = pd.to_datetime(s[miss], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
            "/tmp/ipython-input-2320659374.py:30: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=False, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved /content/data/output/submission.csv | rows=39392 | header=['date','0.02%'] | format=%m/%d/%Y %H:%M | kept old 0/1, filled missing via model, else 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ===== CONFIG =====\n",
        "template_path = \"/content/data/input/emory/predictions.csv\"  # file nguồn có cột 'date'\n",
        "out_dir = Path(\"/content/data/output\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "save_path = out_dir / \"submission.csv\"\n",
        "THRESH = 0.5\n",
        "EXPECTED_ROWS = 39392\n",
        "\n",
        "def to_datetime_flex(series_like):\n",
        "    s = pd.Series(series_like, copy=False).astype(str).str.strip()\n",
        "    s = s.str.replace(r'(\\d{1,2})-(AM|PM)\\b', r'\\1:00 \\2', regex=True, flags=re.IGNORECASE)\n",
        "    s = s.str.replace(r'\\b(\\d{1,2})\\s*(AM|PM)\\b', r'\\1:00 \\2', regex=True, flags=re.IGNORECASE)\n",
        "    s = s.str.replace(r'\\b(\\d{1,2}):(\\d{2})(AM|PM)\\b', r'\\1:\\2 \\3', regex=True, flags=re.IGNORECASE)\n",
        "    dt = pd.to_datetime(s, errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
        "    miss = dt.isna()\n",
        "    if miss.any():\n",
        "        dt2 = pd.to_datetime(s[miss], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
        "        dt[miss] = dt2\n",
        "    return dt.dt.floor('min')\n",
        "\n",
        "def to_binary_or_zero(series):\n",
        "    \"\"\"0/1 giữ nguyên; nếu là số thực -> threshold; còn lại/NaN -> 0.\"\"\"\n",
        "    arr = pd.to_numeric(series, errors='coerce')\n",
        "    if arr.notna().all():\n",
        "        uniq = set(np.unique(arr.values))\n",
        "        if uniq.issubset({0,1}):\n",
        "            return arr.fillna(0).astype(int)\n",
        "        return (arr >= THRESH).astype(int)\n",
        "    return pd.Series(np.zeros(len(series), dtype=int), index=series.index)\n",
        "\n",
        "# ===== 1) Đọc template & xác nhận kích thước =====\n",
        "tpl = pd.read_csv(template_path)\n",
        "date_raw = tpl.iloc[:, 0].astype(str)     # giữ nguyên chuỗi 'date'\n",
        "n = len(tpl)\n",
        "if n != EXPECTED_ROWS:\n",
        "    print(f\"⚠️ File nguồn có {n} dòng (mong đợi {EXPECTED_ROWS}). Vẫn xử lý theo {n} dòng.\")\n",
        "\n",
        "# Nếu file nguồn đã có cột dự đoán, dùng để \"giữ nguyên\" nửa trên\n",
        "if '0.02%' in tpl.columns:\n",
        "    src_pred = to_binary_or_zero(tpl['0.02%'])\n",
        "elif 'target' in tpl.columns:\n",
        "    src_pred = to_binary_or_zero(tpl['target'])\n",
        "elif tpl.shape[1] >= 2:\n",
        "    src_pred = to_binary_or_zero(tpl.iloc[:, 1])\n",
        "else:\n",
        "    src_pred = pd.Series(np.zeros(n, dtype=int))  # không có cột dự đoán -> nửa trên sẽ là 0\n",
        "\n",
        "# ===== 2) Chuẩn bị map dự đoán từ model (chỉ dùng cho nửa dưới) =====\n",
        "# Parse datetime chỉ để map, không đổi output 'date'\n",
        "date_dt = to_datetime_flex(date_raw)\n",
        "\n",
        "X_norm = X_imp.copy()\n",
        "X_norm.index = to_datetime_flex(pd.Series(X_norm.index))\n",
        "\n",
        "need_dt = pd.to_datetime(date_dt.dropna().unique())\n",
        "X_test = X_norm.loc[X_norm.index.isin(need_dt)].copy()\n",
        "\n",
        "if hasattr(model, \"predict_proba\"):\n",
        "    proba = model.predict_proba(X_test)[:, 1]\n",
        "    model_bin = (proba >= THRESH).astype(int)\n",
        "else:\n",
        "    raw = np.asarray(model.predict(X_test)).ravel()\n",
        "    model_bin = raw.astype(int) if set(np.unique(raw)).issubset({0,1}) else (raw >= THRESH).astype(int)\n",
        "\n",
        "pred_map = pd.Series(model_bin, index=pd.to_datetime(X_test.index))\n",
        "pred_map = pred_map[~pred_map.index.duplicated(keep='last')]\n",
        "\n",
        "# ===== 3) Chỉ predict các dòng có số thứ tự > 50% =====\n",
        "half = n // 2  # 0..half-1 giữ nguyên, half..n-1 predict\n",
        "out_vals = src_pred.copy().astype(int).values  # mặc định giữ nguyên\n",
        "\n",
        "for i in range(half, n):\n",
        "    ts = date_dt.iloc[i]\n",
        "    out_vals[i] = int(pred_map.get(ts, 0))  # không map được -> 0\n",
        "\n",
        "# ===== 4) Kết quả & lưu =====\n",
        "out = pd.DataFrame({\n",
        "    'date': date_raw,         # giữ nguyên chuỗi date như file nguồn\n",
        "    '0.02%': out_vals.astype(int)\n",
        "})\n",
        "\n",
        "# đảm bảo số dòng đúng bằng file nguồn (thường là 39,392)\n",
        "if len(out) > EXPECTED_ROWS:\n",
        "    out = out.iloc[:EXPECTED_ROWS].copy()\n",
        "elif len(out) < EXPECTED_ROWS:\n",
        "    print(f\"⚠️ Output có {len(out)} dòng < {EXPECTED_ROWS}. (Giữ nguyên bằng số dòng nguồn).\")\n",
        "\n",
        "out.to_csv(save_path, index=False)\n",
        "print(f\"✅ Saved {save_path} | rows={len(out)} | chỉ predict nửa dưới ({n-half} dòng); nửa trên giữ nguyên; header=['date','0.02%']\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8jwQ9PBoNx",
        "outputId": "67149e2b-5cfd-45b4-b2fa-f20973d42897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3085548020.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
            "/tmp/ipython-input-3085548020.py:21: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt2 = pd.to_datetime(s[miss], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
            "/tmp/ipython-input-3085548020.py:21: UserWarning: Parsing dates in %Y-%m-%d %I:%M:%S %p format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
            "  dt2 = pd.to_datetime(s[miss], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
            "/tmp/ipython-input-3085548020.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=False, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved /content/data/output/submission.csv | rows=39392 | chỉ predict nửa dưới (19696 dòng); nửa trên giữ nguyên; header=['date','0.02%']\n"
          ]
        }
      ]
    }
  ]
}